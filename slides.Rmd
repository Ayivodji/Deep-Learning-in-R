---
title: "Introduction to Deep Learning in R"
subtitle: ""
author: "D-Lab - Chris Kennedy and Evan Muzzall"
date: "Dec 14, 2018 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["robot"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

class: center, middle

# "It’s tough to make predictions, especially about the future." -Yogi Berra

---

# Software requirements

[Click here](https://github.com/dlab-berkeley/Deep-Learning-in-R) to download the workshiop materials
  * Click green “Clone or Download” button
  * Click “Download Zip”
  * Extract this zip file to your desktop

Git users use bash to git clone: `$ git clone git@github.com:dlab-berkeley/Deep-Learning-in-R.git`

[Install R 3.4 or greater](https://cloud.r-project.org/)   

[Install Rstudio](https://www.rstudio.com/products/rstudio/download/)  

[Install Anaconda Python (3.7+)](https://www.anaconda.com/download/#macos) if you have not already done so. Use default options.  

---

# [Install keras] by typing in RStudio script/console:

`devtools::install_github("rstudio/keras")`  
`install.packages("keras")`  
`keras::install_keras()`  

Install key additional packages
Image Magick
install.packages("magick")
library(magick)
(MacOS Homebrew or Linux users, see extra install info here)

---

class: left, middle

# Download the workshop materials 

### 1) Download the workshiop materials: https://github.com/dlab-berkeley/Deep-Learning-in-R

### 2) Download R >= 3.4: https://cloud.r-project.org/  

### 3) Download RStudio Desktop Open Source License FREE: https://www.rstudio.com/products/rstudio/download/

---

class: left

# Workshop Goals

### Introduction  
### Part 1. MNIST handwritten digit classification example  
### Part 2. Dogs-humans example  
### Part 3. Biomedical example  
### Part 4. Cloud example

---

# What is deep learning? (long)

#### Deep Learning allows "computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept deﬁned through its relation to simpler concepts. By gathering knowledge from experience, this approach avoids the need for human operators to formally specify all the knowledge that the computer needs.  

#### The hierarchy of concepts enables the computer to learn complicated concepts bybuilding them out of simpler ones. If we draw a graph showing how these concepts are built on top of each other, the graph is deep, with many layers. For this reason, we call this approach to AI deep learning."  

#### (https://www.deeplearningbook.org/contents/intro.html, pp. 1-2)

---

# What is deep learning? (short)

### - A subfield of machine learning that utilizes multi-layered artificial neural networks for modelling and prediction of data. 

### - These models can be applied to numeric, categorical, image, text, and time-series data. 

### - For images, neural networks convert the image to a matrix of pixel values. Therefore it is useful to think of images as giant matrices! 

---

class: center

# What is an artificial neural network? 

"Perceptron"
```{r, out.width = "600px", echo = F}
knitr::include_graphics("slide_img/ann.png")
```

---

class: center

# What is an activation function?

```{r, out.width = "500px", echo = F}
knitr::include_graphics("slide_img/actfun.png")
```

---

class: center

# What makes a neural network "deep"?

```{r, out.width = "700px", echo = F}
knitr::include_graphics("slide_img/deep.png")
```

---

# So how does it work? 

#### 1. Supervised neural networks require "features", or an **input layer** of data that is used to produce our estimated **output layer**, or estimated classification of an actual image.  

#### 2. This input layer is initialized with a vector of randomized weights and a bias term of "1" is added. 

#### 3. The products of these input features and their weights are summed and  transformed. This summed value is passed through an activation function to determine the threshold if our prediction should be classified as a 0 or a 1. A learning rate is also be specified. 

---

# So how does it work? 

#### 4. Error is computed as target value minus our estimation. The weights delta metric (how much the errors should change) is calculated as the error times the slope of the point on the activation function times the vector of input features. 

#### 5. The vector of original weights is added to the vector of updated weights and are "backpropagated" (used as the recycled input) and passed through the model for another epoch.  

#### 6. With deep networks, the process also takes places between **hidden layers**, or areas of nonlinear transformations connected only to the layers before and after them. They are referred to as "hidden" because they are not show as the final output. 

---

# Acknowledgements  
Allaire JJ. 2017. [keras for R](https://blog.rstudio.com/2017/09/05/keras-for-r/)

Allaire JJ, Chollet F. 2018. keras: R interface to 'Keras'.
- https://keras.rstudio.com

- https://cran.r-project.org/web/packages/keras/vignettes/getting_started.html

Dancho M. 2018. [Deep learning with keras to predict customer churn.](https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/)

[Getting started](https://cran.r-project.org/web/packages/keras/vignettes/getting_started.html) with keras: Overview vignette. 

Lakhani, P. 2018. [Hello_World_Deep_Learning.](https://github.com/paras42/Hello_World_Deep_Learning)

[View the resouces listed](https://github.com/dlab-berkeley/Deep-Learning-in-R/blob/master/README.md) in this repository's readme file. 

Images [borrowed from Qingkai Kong's ANNs in Python workshop.](https://github.com/qingkaikong/20181129_ANN_basics_DLab)