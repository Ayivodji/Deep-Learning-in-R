---
title: "Introduction to Deep Learning in R - Part 1"
author: "D-Lab"
date: "11/24/2018"
output: html_document
---

## Introduction
(Slideshow presentation)

Goals

## Install packages

Run this chunk manually to install once. It will not be run when one clicks "knit" or "run all".

```{r install, eval = FALSE}

# Install keras package
devtools::install_github("rstudio/keras")
# or
install.packages("keras")

# Then run install_keras() function to install anaconda python, tensorflow, and keras
keras::install_keras()
# Or if you have a GPU and have followed these instructions:
# https://tensorflow.rstudio.com/tools/local_gpu.html
# NOTE: tensorflow seems to require CUDA 9.0 currently; 9.2 for example will not work.
# keras::install_keras(tensorflow = "gpu")

# Customize EBImage installation to R version.
if (grepl("version 3\\.5\\.", R.version.string)) {
  # R >= 3.5
  install.packages("BiocManager")
  BiocManager::install("EBImage")
} else {
  # R < 3.5
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}


```

## Load packages
Now that we have installed the necessary packages, library them so that R can utilize their functionalities

```{r load_packages}

library(cowplot)
library(keras)
library(dplyr)
library(tensorflow)
# Installed via bioconductor above, not CRAN.
library(EBImage)
library(ggplot2)

```

## MNIST handwritten digit example
Jump in! The first example will consist of a walkthrough of the Keras vignette [located here](https://cran.r-project.org/web/packages/keras/vignettes/getting_started.html).  

Let's look at 70,000 handwritten digit images from the [Modified National Institute of Standards and Technology database](https://en.wikipedia.org/wiki/MNIST_database) (MNIST). 

```{r}
# This line requires a working internet connection to download the data the first time.
mnist <- dataset_mnist()

# How many images are in our training and test sets? What are their pixel dimensions? 

# Define our x and y variables for the training and test sets. 
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
```

## Reshape, rescale, one-hot encode

The `array_reshape` function allows us to reshape a three-dimensional array like those foudn in our `mnist` dataset into matrices. Our 28x28 pixel images will become vectors with length $28*28 = 784$. 

-dimensional array. This is critical since we want to turn our matrices 

```{r}
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))

# Check dimensions of x.
dim(x_train)
dim(x_test)
```

Then, grayscale values that range from between 0 to 255 are scaled 
```{r}
# rescale
x_train <- x_train / 255
x_test <- x_test / 255

# Convert categorical vector into a one-hot encoded matrix.
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)

# Check dimensions of new y matrices.
dim(y_train)
dim(y_test)

# Review percentage of obs with each digit label.
round(colMeans(y_train), 3)
round(colMeans(y_test), 3)
```

## Define the model

```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')
summary(model)
```

## Compile model with loss function, optimizer, and metrics

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

## Train and evaluate

```{r}
# Watch the model build epoch by epoch
set.seed(1)
history <- model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)
```

## Plot history via ggplot

```{r}
plot(history) + theme_minimal()
```

## Evaluate performance on the test data

```{r}
model %>% evaluate(x_test, y_test)
```
