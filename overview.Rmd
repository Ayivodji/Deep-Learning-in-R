---
title: "Introduction to Deep Learning in R"
author: "Evan"
date: "10/4/2018"
output: html_document
---

Should we set up datahub for this workshops? Seems like there could be a lot of install issues... Are there any known dependency issues?

**CK: I haven't used datahub, what does it entail?**

## Install dependencies

Run this chunk manually to install once. It will not be run when one clicks "knit" or "run all".

```{r install, eval = FALSE}

# Install keras
install.packages("keras")
keras::install_keras()

# Customize EBImage installation to R version.
if (grepl("version 3\\.5\\.", R.version.string)) {
  # R >= 3.5
  install.packages("BiocManager")
  BiocManager::install("EBImage")
} else {
  # R < 3.5
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}


```

## Load packages

```{r load_packages}

library(keras)
library(dplyr)
# Installed via bioconductor above, not CRAN.
library(EBImage)
library(ggplot2)

```

## Keras vignette

[located here](https://cran.r-project.org/web/packages/keras/vignettes/getting_started.html)
Load and split data

```{r}
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
```

## Reshape, rescale, one-hot encode

```{r}
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))

# rescale
x_train <- x_train / 255
x_test <- x_test / 255

# one-hot encode
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

## Define the model

```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')
summary(model)
```

## Compile model with loss function, optimizer, and metrics

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

## Train and evaluate

```{r}
# Watch the model build epoch by epoch
set.seed(1)
history <- model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)
```

## Plot history via ggplot

Why do people like such ugly ggplots?

```{r}
plot(history) + theme_minimal()
```

## Evaluate performance on the test data

```{r}
model %>% evaluate(x_test, y_test)
```

## Humans and dogs

### The data

Load a folder of human and dog images from the `data` folder of the workshop materials and define the vector of `image_names` to be populated in a blank list named `images_list`. These free images were downloaded from [Burst](https://burst.shopify.com/):

### Easy data import

Loading image data and standardizing features for each image can be intimidating. Fortunately, we have written a for-loop that does everything for you. Just make sure you set your working directory to the `data` folder inside of the reposistory that you downloaded for this workshop.  

> **From the RStudio toolbar, click "Session" --> "Set Working Directory" --> "Choose Directory" to set your working directory.** A diagoglue box will appear - just click the "data" folder and "open" to proceed.  

*CK: we should not change the working directory, instead use a relative file path like "data/blah.png".*

Write a for-loop that imports the images, standardizes their sizes, prints summary information for each, and plots each image. This loop is stored instide the `ez_import` function. This function takes no arguments and accomplishes the following:  

1. Imports images from the working directory  
2. Resize images to a standard size  
3. Adds a header for the summary information for each image  
4. Plots each image  

```{r}
# Define ez_import function

# for now, file_path can only be set to getwd(), so just be sure to set your working directory to the data folder first! 
ez_import = function(file_path, width = 224, height = 224, to_grayscale = TRUE, verbose = FALSE) {
  
  # Store file names in a vector
  images = list.files(file_path, full.names = T)
  
  # Create a blank list to be appended
  images_list = list()
  
  # Build the for-loop
  for (i in seq_along(images)) {
    
    # Import images
    images_list[[i]] = EBImage::readImage(images[i])
    
    # Resize images (e.g., 224 x 224 dpi)
    images_list[[i]] = EBImage::resize(images_list[[i]], w = width, h = height)
    
    # Convert to grayscale
    if (to_grayscale) {
      images_list[[i]] = EBImage::channel(images_list[[i]], mode = "gray")
    }
    
    # Name each image in the output for reference
    if (verbose) {
      cat("image #", i, "-", images[[i]],
          "\n")
    }
    
    # Display color profile values
    if (verbose) {
      print(images_list[[i]])
    }
    
    # Plot each image. Click left and right arrows in plot tab to view.
    plot(images_list[[i]])
    
    # Garbage collection
    gc()
  }
    
  # Assign binary sparse matrices for each image
  create_array = array(as.numeric(unlist(images_list)), dim = c(length(images), width, height))
  sparse_matrix = ifelse(create_array > 0.5, 1, 0)

  # Display number of images processed
  cat("images processed:", length(images),
      "\n")
    
  # Save output
  results = list(
    images = images_list,
    array = create_array, 
    sparse = sparse_matrix)
    
  return(results)
  
}
```

## Run the function to load data

```{r}
# ez_import(getwd())
ez_import("data", verbose = T, 256, 256)
```

## display() will also show image in "Viewer" tab

```{r}
display(image_data[[1]])

# or

plot(image_data[[1]])
```

## Training/testing on this small dataset

### Split data

```{r}
# Choose the middle 30 images as the training data
x_train = binary_data[11:40,,]
(y_train = c(rep("dog", 15), rep("human", 15)))
(y_train = ifelse(y_train == "dog", 1, 0))

# Choose the first 10 dogs and last 10 humans as the test data
x_test = binary_data[c(1:10, 41:50),,]
(y_test = c(rep("dog", 10), rep("human", 10)))
(y_test = ifelse(y_test == "dog", 1, 0))
```

### Reshape sparse matrices to arrays

```{r}
x_train = array_reshape(x_train, c(nrow(x_train), 224*224))
x_test = array_reshape(x_test, c(nrow(x_test), 224*224))

# rescale
x_train = x_train / 255
x_test = x_test / 255

# 
y_train <- to_categorical(y_train, 2)
y_test <- to_categorical(y_test, 2)
```

## Define the model

```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 200, activation = 'relu', input_shape = 50176) %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 50, activation = 'relu') %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 2, activation = 'sigmoid')
summary(model)
```

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

```{r}
set.seed(1)
history <- model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 10, 
  validation_split = 0.2
)
```

```{r}
plot(history) + theme_bw()
```

```{r}
model %>% evaluate(x_test, y_test)
```

## Repeat on biomedical image dataset

(coming soon)

## Resources

(Add links)